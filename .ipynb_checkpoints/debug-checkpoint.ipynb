{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src\n",
    "import numpy as np\n",
    "import argparse\n",
    "import copy\n",
    "import logging\n",
    "import random\n",
    "import torch\n",
    "from box import Box\n",
    "from pathlib import Path\n",
    "from shutil import copyfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_instance(module, config, *args):\n",
    "\n",
    "    cls = getattr(module, config.name)\n",
    "    return cls(*args, **config.get('kwargs', {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Box.from_yaml(filename='/home/tony/Incremental_Learning/configs/train/icarl_config.yaml')\n",
    "saved_dir = Path(config.main.saved_dir)\n",
    "if not saved_dir.is_dir():\n",
    "    saved_dir.mkdir(parents=True)\n",
    "\n",
    "random_seed = config.main.get('random_seed')\n",
    "if random_seed is None:\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    logging.info('Make the experiment results deterministic.')\n",
    "    random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "logging.info('Create the device.')\n",
    "if 'cuda' in config.trainer.kwargs.device and not torch.cuda.is_available():\n",
    "    raise ValueError(\"The cuda is not available. Please set the device to 'cpu'.\")\n",
    "device = torch.device(config.trainer.kwargs.device)\n",
    "\n",
    "logging.info('Create the network architecture.')\n",
    "config.net.setdefault('kwargs', {}).update(device=device)\n",
    "net = _get_instance(src.model.nets, config.net).to(device)\n",
    "\n",
    "logging.info('Create the loss functions and corresponding weights.')\n",
    "loss_fns, loss_weights = [], []\n",
    "defaulted_loss_fns = [loss_fn for loss_fn in dir(torch.nn) if 'Loss' in loss_fn]\n",
    "for config_loss in config.losses:\n",
    "    if config_loss.name in defaulted_loss_fns:\n",
    "        loss_fn = _get_instance(torch.nn, config_loss)\n",
    "    else:\n",
    "        loss_fn = _get_instance(src.model.losses, config_loss)\n",
    "    loss_fns.append(loss_fn)\n",
    "    loss_weights.append(config_loss.weight)\n",
    "\n",
    "logging.info('Create the metric functions.')\n",
    "metric_fns = [_get_instance(src.model.metrics, config_metric) for config_metric in config.metrics]\n",
    "\n",
    "logging.info('Create the learner.')\n",
    "kwargs = {\n",
    "    'config': config,\n",
    "    'saved_dir': saved_dir,\n",
    "    'device': device,\n",
    "    'net': net,\n",
    "    'loss_fns': loss_fns,\n",
    "    'loss_weights': loss_weights,\n",
    "    'metric_fns': metric_fns,\n",
    "}\n",
    "config.learner.kwargs.update(kwargs)\n",
    "\n",
    "learner = _get_instance(src.learner, config.learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "[450. 450. 450. 450. 450. 450. 450. 450. 450. 450.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.]\n",
      "4\n",
      "[50. 50. 50. 50. 50. 50. 50. 50. 50. 50.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "imgs = []\n",
    "learner.current_task += 1\n",
    "dataset = learner.build_dataset(learner.config, learner.current_task, learner.class_per_task)\n",
    "loader = learner.build_dataloader(learner.config, dataset, 'train')\n",
    "print(len(loader))\n",
    "count = np.zeros(100)\n",
    "for data in loader:\n",
    "    for i in range(data['targets'].size(0)):\n",
    "        gt = data['targets'][i, 0]\n",
    "        if gt==5:\n",
    "            imgs.append(data['inputs'])\n",
    "        count[gt] += 1\n",
    "print(count)\n",
    "loader = learner.build_dataloader(learner.config, dataset, 'valid')\n",
    "print(len(loader))\n",
    "count = np.zeros(100)\n",
    "for data in loader:\n",
    "    for i in range(data['targets'].size(0)):\n",
    "        gt = data['targets'][i, 0]\n",
    "        count[gt] += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = imgs[3]\n",
    "img = img.permute(1, 2, 0).cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n",
      "[100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n",
      " 100. 100.]\n"
     ]
    }
   ],
   "source": [
    "loader = learner.build_dataloader(learner.config, dataset, 'test')\n",
    "print(len(loader))\n",
    "count = np.zeros(100)\n",
    "for data in loader:\n",
    "    for i in range(data['targets'].size(0)):\n",
    "        gt = data['targets'][i, 0]\n",
    "        count[gt] += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "tensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
